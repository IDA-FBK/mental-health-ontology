{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIAO Automatic Modeling: Depression Severity Detection\n",
    "\n",
    "This notebook demonstrates automatic generation of MIAO-compliant RDF annotations from:\n",
    "1. A text dataset with ordinal depression severity labels (0=Minimal, 1=Mild, 2=Moderate, 3=Severe)\n",
    "2. Machine learning experiment results from multiple models (BERT, Feature Framework)\n",
    "\n",
    "The notebook creates a complete RDF knowledge graph following the MIAO ontology structure for depression severity research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install rdflib pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from rdflib import Graph, Namespace, Literal, URIRef, RDF, RDFS, XSD\n",
    "from rdflib.namespace import DCTERMS, PROV\n",
    "import hashlib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MIAO and related namespaces\n",
    "MIAO = Namespace(\"https://w3id.org/miao#\")\n",
    "MLS = Namespace(\"http://www.w3.org/ns/mls#\")\n",
    "EX = Namespace(\"https://w3id.org/miao/depression-experiment#\")\n",
    "\n",
    "# Create RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"miao\", MIAO)\n",
    "g.bind(\"mls\", MLS)\n",
    "g.bind(\"ex\", EX)\n",
    "g.bind(\"dcterms\", DCTERMS)\n",
    "g.bind(\"prov\", PROV)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"xsd\", XSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Input Data\n",
    "\n",
    "### 3.1 Load Depression Severity Dataset\n",
    "\n",
    "Expected format:\n",
    "- CSV file with columns: `text`, `severity_label`\n",
    "- `severity_label`: 0 (Minimal), 1 (Mild), 2 (Moderate), 3 (Severe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 samples\n",
      "\n",
      "Severity distribution:\n",
      "severity_name\n",
      "Mild        2\n",
      "Minimal     3\n",
      "Moderate    3\n",
      "Severe      2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>severity_label</th>\n",
       "      <th>severity_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feeling okay today, nothing special but managi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sometimes I feel a bit down but it passes quickly</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having trouble getting out of bed, everything ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't remember the last time I felt happy, c...</td>\n",
       "      <td>3</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Life is good, enjoying my hobbies and social a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Minimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  severity_label  \\\n",
       "0  Feeling okay today, nothing special but managi...               0   \n",
       "1  Sometimes I feel a bit down but it passes quickly               1   \n",
       "2  Having trouble getting out of bed, everything ...               2   \n",
       "3  I can't remember the last time I felt happy, c...               3   \n",
       "4  Life is good, enjoying my hobbies and social a...               0   \n",
       "\n",
       "  severity_name  \n",
       "0       Minimal  \n",
       "1          Mild  \n",
       "2      Moderate  \n",
       "3        Severe  \n",
       "4       Minimal  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset (replace with your actual file path)\n",
    "# For demonstration, we'll create a sample dataset\n",
    "\n",
    "# Option 1: Load from CSV\n",
    "# df_dataset = pd.read_csv('depression_severity_dataset.csv')\n",
    "\n",
    "# Option 2: Create sample data for demonstration\n",
    "df_dataset = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Feeling okay today, nothing special but managing fine\",\n",
    "        \"Sometimes I feel a bit down but it passes quickly\",\n",
    "        \"Having trouble getting out of bed, everything feels pointless\",\n",
    "        \"I can't remember the last time I felt happy, constant sadness\",\n",
    "        \"Life is good, enjoying my hobbies and social activities\",\n",
    "        \"Occasional low mood but generally functioning well\",\n",
    "        \"Struggling with daily tasks, feeling hopeless about the future\",\n",
    "        \"Severe symptoms affecting my work, relationships, and self-care\",\n",
    "        \"Minimal symptoms, coping well with stress\",\n",
    "        \"Moderate depression interfering with some aspects of my life\"\n",
    "    ],\n",
    "    'severity_label': [0, 1, 2, 3, 0, 1, 2, 3, 0, 2]  # 0=Minimal, 1=Mild, 2=Moderate, 3=Severe\n",
    "})\n",
    "\n",
    "# Map labels to severity names\n",
    "severity_map = {0: 'Minimal', 1: 'Mild', 2: 'Moderate', 3: 'Severe'}\n",
    "df_dataset['severity_name'] = df_dataset['severity_label'].map(severity_map)\n",
    "\n",
    "print(f\"Loaded {len(df_dataset)} samples\")\n",
    "print(f\"\\nSeverity distribution:\")\n",
    "print(df_dataset['severity_name'].value_counts().sort_index())\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load ML Experiment Results\n",
    "\n",
    "Expected format:\n",
    "- DataFrame with columns: `sample_id`, `model_name`, `predicted_label`, `confidence`, `true_label`\n",
    "- Support for multiple models (BERT, Feature Framework, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 predictions from 2 models\n",
      "\n",
      "Accuracy by model:\n",
      "  BERT_Depression_Classifier: 100.00%\n",
      "  Feature_Framework_Classifier: 90.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.91</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>3</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>BERT_Depression_Classifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                  model_name  predicted_label  confidence  \\\n",
       "0          0  BERT_Depression_Classifier                0        0.89   \n",
       "1          1  BERT_Depression_Classifier                1        0.76   \n",
       "2          2  BERT_Depression_Classifier                2        0.82   \n",
       "3          3  BERT_Depression_Classifier                3        0.91   \n",
       "4          4  BERT_Depression_Classifier                0        0.93   \n",
       "5          5  BERT_Depression_Classifier                1        0.71   \n",
       "6          6  BERT_Depression_Classifier                2        0.78   \n",
       "7          7  BERT_Depression_Classifier                3        0.88   \n",
       "8          8  BERT_Depression_Classifier                0        0.90   \n",
       "9          9  BERT_Depression_Classifier                2        0.80   \n",
       "\n",
       "   true_label  \n",
       "0           0  \n",
       "1           1  \n",
       "2           2  \n",
       "3           3  \n",
       "4           0  \n",
       "5           1  \n",
       "6           2  \n",
       "7           3  \n",
       "8           0  \n",
       "9           2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1: Load from CSV\n",
    "# df_results = pd.read_csv('experiment_results.csv')\n",
    "\n",
    "# Option 2: Create sample results for demonstration (multiple models)\n",
    "np.random.seed(42)\n",
    "\n",
    "# BERT model results\n",
    "bert_results = pd.DataFrame({\n",
    "    'sample_id': range(10),\n",
    "    'model_name': 'BERT_Depression_Classifier',\n",
    "    'predicted_label': [0, 1, 2, 3, 0, 1, 2, 3, 0, 2],\n",
    "    'confidence': [0.89, 0.76, 0.82, 0.91, 0.93, 0.71, 0.78, 0.88, 0.90, 0.80],\n",
    "    'true_label': df_dataset['severity_label'].values\n",
    "})\n",
    "\n",
    "# Feature Framework results\n",
    "framework_results = pd.DataFrame({\n",
    "    'sample_id': range(10),\n",
    "    'model_name': 'Feature_Framework_Classifier',\n",
    "    'predicted_label': [0, 1, 2, 3, 0, 1, 2, 2, 0, 2],  # One misclassification\n",
    "    'confidence': [0.85, 0.79, 0.84, 0.87, 0.91, 0.74, 0.81, 0.75, 0.88, 0.83],\n",
    "    'true_label': df_dataset['severity_label'].values\n",
    "})\n",
    "\n",
    "df_results = pd.concat([bert_results, framework_results], ignore_index=True)\n",
    "\n",
    "# Model performance metrics (aggregate)\n",
    "model_metrics = {\n",
    "    'BERT_Depression_Classifier': {\n",
    "        'implementation': 'PyTorch',\n",
    "        'version': '1.0',\n",
    "        'architecture': 'BERT-base fine-tuned',\n",
    "        'accuracy': 0.90,\n",
    "        'macro_precision': 0.88,\n",
    "        'macro_recall': 0.89,\n",
    "        'macro_f1': 0.88,\n",
    "        'weighted_f1': 0.90,\n",
    "        'training_date': '2025-11-20',\n",
    "        'hyperparameters': {\n",
    "            'learning_rate': 2e-5,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 10,\n",
    "            'max_length': 512\n",
    "        }\n",
    "    },\n",
    "    'Feature_Framework_Classifier': {\n",
    "        'implementation': 'scikit-learn',\n",
    "        'version': '1.0',\n",
    "        'architecture': 'SVM with LIWC features and GloVe embeddings',\n",
    "        'accuracy': 0.85,\n",
    "        'macro_precision': 0.83,\n",
    "        'macro_recall': 0.84,\n",
    "        'macro_f1': 0.83,\n",
    "        'weighted_f1': 0.85,\n",
    "        'training_date': '2025-11-20',\n",
    "        'hyperparameters': {\n",
    "            'kernel': 'rbf',\n",
    "            'C': 1.0,\n",
    "            'gamma': 'scale',\n",
    "            'embedding_dim': 300\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(df_results)} predictions from {df_results['model_name'].nunique()} models\")\n",
    "print(f\"\\nAccuracy by model:\")\n",
    "for model in df_results['model_name'].unique():\n",
    "    model_data = df_results[df_results['model_name'] == model]\n",
    "    acc = (model_data['predicted_label'] == model_data['true_label']).mean()\n",
    "    print(f\"  {model}: {acc:.2%}\")\n",
    "\n",
    "df_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create MIAO Depression Severity Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created schema: https://w3id.org/miao/depression-experiment#DepressionSeveritySchema_Research\n",
      "\n",
      "Categories:\n",
      "  0: https://w3id.org/miao/depression-experiment#Minimal\n",
      "  1: https://w3id.org/miao/depression-experiment#Mild\n",
      "  2: https://w3id.org/miao/depression-experiment#Moderate\n",
      "  3: https://w3id.org/miao/depression-experiment#Severe\n"
     ]
    }
   ],
   "source": [
    "def create_depression_severity_schema(graph):\n",
    "    \"\"\"\n",
    "    Create ordinal depression severity classification schema in MIAO format.\n",
    "    \"\"\"\n",
    "    # Define schema\n",
    "    schema_uri = EX.DepressionSeveritySchema_Research\n",
    "    graph.add((schema_uri, RDF.type, MIAO.MentalIllnessesSchema))\n",
    "    graph.add((schema_uri, DCTERMS.title, \n",
    "               Literal(\"Research schema for depression severity in text\", lang=\"en\")))\n",
    "    graph.add((schema_uri, DCTERMS.description, \n",
    "               Literal(\"Ordinal taxonomy of depression severity (Minimal, Mild, Moderate, Severe) used in social media corpora for computational research on depression detection\", lang=\"en\")))\n",
    "    graph.add((schema_uri, DCTERMS.created, \n",
    "               Literal(datetime.now().strftime(\"%Y-%m-%d\"), datatype=XSD.date)))\n",
    "    \n",
    "    # Define categories with PHQ-9 score ranges\n",
    "    categories = [\n",
    "        (0, \"Minimal\", \"Minimal or no depressive symptoms. PHQ-9 score range: 0-4.\"),\n",
    "        (1, \"Mild\", \"Mild depressive symptoms. PHQ-9 score range: 5-9.\"),\n",
    "        (2, \"Moderate\", \"Moderate depressive symptoms. PHQ-9 score range: 10-14.\"),\n",
    "        (3, \"Severe\", \"Severe depressive symptoms. PHQ-9 score range: 15-27.\")\n",
    "    ]\n",
    "    \n",
    "    category_map = {}\n",
    "    \n",
    "    for level, name, description in categories:\n",
    "        category_uri = EX[name]\n",
    "        graph.add((category_uri, RDF.type, MIAO.MentalIllnessCategory))\n",
    "        graph.add((category_uri, DCTERMS.title, Literal(f\"{name} depression\", lang=\"en\")))\n",
    "        graph.add((category_uri, DCTERMS.description, Literal(description, lang=\"en\")))\n",
    "        graph.add((category_uri, DCTERMS.identifier, Literal(str(level), datatype=XSD.integer)))\n",
    "        graph.add((category_uri, MIAO.isMentalIllnessCategoryOf, schema_uri))\n",
    "        graph.add((schema_uri, MIAO.hasMentalIllnessCategory, category_uri))\n",
    "        graph.add((category_uri, RDFS.label, Literal(f\"{name} Depression\", lang=\"en\")))\n",
    "        \n",
    "        category_map[level] = category_uri\n",
    "    \n",
    "    return schema_uri, category_map\n",
    "\n",
    "schema_uri, category_map = create_depression_severity_schema(g)\n",
    "print(f\"Created schema: {schema_uri}\")\n",
    "print(f\"\\nCategories:\")\n",
    "for level, uri in category_map.items():\n",
    "    print(f\"  {level}: {uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Dataset as MIAO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset: https://w3id.org/miao/depression-experiment#DepressionSeverityTextDataset\n"
     ]
    }
   ],
   "source": [
    "def create_dataset_metadata(graph, df, dataset_name=\"DepressionSeverityTextDataset\"):\n",
    "    \"\"\"\n",
    "    Create dataset metadata in MLS format.\n",
    "    \"\"\"\n",
    "    dataset_uri = EX[dataset_name]\n",
    "    graph.add((dataset_uri, RDF.type, MLS.Dataset))\n",
    "    graph.add((dataset_uri, RDFS.label, \n",
    "               Literal(f\"{dataset_name} - Social media text for depression severity detection\", lang=\"en\")))\n",
    "    graph.add((dataset_uri, DCTERMS.description, \n",
    "               Literal(f\"Dataset containing {len(df)} social media text samples with ordinal depression severity annotations (Minimal, Mild, Moderate, Severe)\", lang=\"en\")))\n",
    "    graph.add((dataset_uri, DCTERMS.extent, \n",
    "               Literal(f\"{len(df)} samples\", lang=\"en\")))\n",
    "    graph.add((dataset_uri, DCTERMS.format, Literal(\"text/plain\")))\n",
    "    graph.add((dataset_uri, DCTERMS.created, \n",
    "               Literal(datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"), datatype=XSD.dateTime)))\n",
    "    graph.add((dataset_uri, DCTERMS.source, Literal(\"Reddit (r/depression, r/mentalhealth)\", lang=\"en\")))\n",
    "    \n",
    "    # Add severity distribution statistics\n",
    "    for level in range(4):\n",
    "        count = (df['severity_label'] == level).sum()\n",
    "        severity_name = severity_map[level]\n",
    "        graph.add((dataset_uri, EX[f\"{severity_name.lower()}Count\"], \n",
    "                   Literal(int(count), datatype=XSD.integer)))\n",
    "    \n",
    "    return dataset_uri\n",
    "\n",
    "dataset_uri = create_dataset_metadata(g, df_dataset)\n",
    "print(f\"Created dataset: {dataset_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model ML Implementations and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created implementation and model for: BERT_Depression_Classifier\n",
      "Created implementation and model for: Feature_Framework_Classifier\n",
      "\n",
      "Total implementations created: 2\n",
      "Total models created: 2\n"
     ]
    }
   ],
   "source": [
    "def create_ml_implementation(graph, model_name, metrics):\n",
    "    \"\"\"\n",
    "    Create ML implementation and software metadata for a specific model.\n",
    "    \"\"\"\n",
    "    impl_type = metrics['implementation']\n",
    "    \n",
    "    # Software (PyTorch or scikit-learn)\n",
    "    software_uri = EX[impl_type.replace('-', '_')]\n",
    "    graph.add((software_uri, RDF.type, MLS.Software))\n",
    "    graph.add((software_uri, RDFS.label, Literal(impl_type, lang=\"en\")))\n",
    "    \n",
    "    if impl_type == \"PyTorch\":\n",
    "        graph.add((software_uri, DCTERMS.description, \n",
    "                   Literal(\"Open-source machine learning framework\", lang=\"en\")))\n",
    "        graph.add((software_uri, DCTERMS.hasVersion, Literal(\"2.1.0\")))\n",
    "    else:\n",
    "        graph.add((software_uri, DCTERMS.description, \n",
    "                   Literal(\"Machine learning library for Python\", lang=\"en\")))\n",
    "        graph.add((software_uri, DCTERMS.hasVersion, Literal(\"1.3.0\")))\n",
    "    \n",
    "    # Implementation\n",
    "    impl_name = model_name.replace(' ', '_')\n",
    "    impl_uri = EX[f\"{impl_name}_Implementation\"]\n",
    "    graph.add((impl_uri, RDF.type, MLS.Implementation))\n",
    "    graph.add((impl_uri, RDFS.label, \n",
    "               Literal(f\"{model_name} Implementation\", lang=\"en\")))\n",
    "    graph.add((impl_uri, DCTERMS.description, \n",
    "               Literal(metrics['architecture'], lang=\"en\")))\n",
    "    graph.add((software_uri, MLS.hasPart, impl_uri))\n",
    "    \n",
    "    # Hyperparameters\n",
    "    for param_name, param_value in metrics['hyperparameters'].items():\n",
    "        param_uri = EX[f\"{impl_name}_{param_name}\"]\n",
    "        graph.add((param_uri, RDF.type, MLS.HyperParameter))\n",
    "        graph.add((param_uri, RDFS.label, \n",
    "                   Literal(param_name.replace('_', ' ').title(), lang=\"en\")))\n",
    "        \n",
    "        # Determine datatype\n",
    "        if isinstance(param_value, float):\n",
    "            graph.add((param_uri, MLS.hasValue, Literal(param_value, datatype=XSD.float)))\n",
    "        elif isinstance(param_value, int):\n",
    "            graph.add((param_uri, MLS.hasValue, Literal(param_value, datatype=XSD.integer)))\n",
    "        else:\n",
    "            graph.add((param_uri, MLS.hasValue, Literal(str(param_value))))\n",
    "        \n",
    "        graph.add((impl_uri, MLS.hasHyperParameter, param_uri))\n",
    "    \n",
    "    return impl_uri, software_uri\n",
    "\n",
    "def create_trained_model(graph, model_name, metrics, impl_uri):\n",
    "    \"\"\"\n",
    "    Create trained model instance.\n",
    "    \"\"\"\n",
    "    model_name_clean = model_name.replace(' ', '_')\n",
    "    model_uri = EX[f\"{model_name_clean}_v{metrics['version']}\"]\n",
    "    graph.add((model_uri, RDF.type, MLS.Model))\n",
    "    graph.add((model_uri, RDFS.label, \n",
    "               Literal(f\"{model_name} v{metrics['version']}\", lang=\"en\")))\n",
    "    graph.add((model_uri, DCTERMS.created, \n",
    "               Literal(metrics['training_date'], datatype=XSD.date)))\n",
    "    graph.add((model_uri, DCTERMS.description, \n",
    "               Literal(f\"Trained model for 4-class depression severity classification: {metrics['architecture']}\", lang=\"en\")))\n",
    "    \n",
    "    return model_uri\n",
    "\n",
    "# Create implementations and models for all models\n",
    "implementations = {}\n",
    "models = {}\n",
    "\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    impl_uri, software_uri = create_ml_implementation(g, model_name, metrics)\n",
    "    model_uri = create_trained_model(g, model_name, metrics, impl_uri)\n",
    "    implementations[model_name] = impl_uri\n",
    "    models[model_name] = model_uri\n",
    "    print(f\"Created implementation and model for: {model_name}\")\n",
    "\n",
    "print(f\"\\nTotal implementations created: {len(implementations)}\")\n",
    "print(f\"Total models created: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Detection Runs and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created detection run for: BERT_Depression_Classifier (ID: 7c8492f1)\n",
      "Created detection run for: Feature_Framework_Classifier (ID: f4a4e505)\n"
     ]
    }
   ],
   "source": [
    "def create_detection_run(graph, model_name, schema_uri, dataset_uri, impl_uri, model_uri, metrics):\n",
    "    \"\"\"\n",
    "    Create automatic detection run (experiment execution) for a specific model.\n",
    "    \"\"\"\n",
    "    run_id = hashlib.md5(f\"{model_name}_{datetime.now()}\".encode()).hexdigest()[:8]\n",
    "    run_uri = EX[f\"Run_{model_name.replace(' ', '_')}_{run_id}\"]\n",
    "    \n",
    "    graph.add((run_uri, RDF.type, MIAO.AutomaticMentalIllnessesDetection))\n",
    "    graph.add((run_uri, RDF.type, MLS.Run))\n",
    "    graph.add((run_uri, RDFS.label, \n",
    "               Literal(f\"Depression severity detection run: {model_name}\", lang=\"en\")))\n",
    "    graph.add((run_uri, DCTERMS.description, \n",
    "               Literal(f\"Automatic depression severity detection using {model_name}\", lang=\"en\")))\n",
    "    graph.add((run_uri, DCTERMS.created, \n",
    "               Literal(datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"), datatype=XSD.dateTime)))\n",
    "    \n",
    "    # Connect to components\n",
    "    graph.add((run_uri, MIAO.hasInputData, dataset_uri))\n",
    "    graph.add((run_uri, MIAO.usedMentalIllnessesSchema, schema_uri))\n",
    "    graph.add((run_uri, MLS.executes, impl_uri))\n",
    "    \n",
    "    # Create evaluation measures and metrics\n",
    "    evaluation_metrics = {\n",
    "        'accuracy': ('Accuracy', metrics['accuracy']),\n",
    "        'macro_precision': ('Macro Precision', metrics['macro_precision']),\n",
    "        'macro_recall': ('Macro Recall', metrics['macro_recall']),\n",
    "        'macro_f1': ('Macro F1 Score', metrics['macro_f1']),\n",
    "        'weighted_f1': ('Weighted F1 Score', metrics['weighted_f1'])\n",
    "    }\n",
    "    \n",
    "    for metric_key, (metric_label, metric_value) in evaluation_metrics.items():\n",
    "        # Create or reference evaluation measure\n",
    "        measure_uri = EX[metric_key]\n",
    "        if (measure_uri, RDF.type, MLS.EvaluationMeasure) not in graph:\n",
    "            graph.add((measure_uri, RDF.type, MLS.EvaluationMeasure))\n",
    "            graph.add((measure_uri, RDFS.label, Literal(metric_label, lang=\"en\")))\n",
    "        \n",
    "        # Create evaluation instance\n",
    "        eval_uri = EX[f\"{run_id}_{metric_key}_evaluation\"]\n",
    "        graph.add((eval_uri, RDF.type, MLS.ModelEvaluation))\n",
    "        graph.add((eval_uri, MLS.specifiedBy, measure_uri))\n",
    "        graph.add((eval_uri, MLS.hasValue, Literal(metric_value, datatype=XSD.float)))\n",
    "        graph.add((run_uri, MLS.hasOutput, eval_uri))\n",
    "    \n",
    "    graph.add((run_uri, MLS.hasOutput, model_uri))\n",
    "    \n",
    "    return run_uri, run_id\n",
    "\n",
    "# Create detection runs for all models\n",
    "runs = {}\n",
    "run_ids = {}\n",
    "\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    run_uri, run_id = create_detection_run(\n",
    "        g, model_name, schema_uri, dataset_uri, \n",
    "        implementations[model_name], models[model_name], metrics\n",
    "    )\n",
    "    runs[model_name] = run_uri\n",
    "    run_ids[model_name] = run_id\n",
    "    print(f\"Created detection run for: {model_name} (ID: {run_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Individual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 10 predictions for: BERT_Depression_Classifier\n",
      "Created 10 predictions for: Feature_Framework_Classifier\n",
      "\n",
      "Total prediction sets created: 2\n"
     ]
    }
   ],
   "source": [
    "def create_predictions(graph, df_results, df_dataset, model_name, run_uri, run_id, category_map):\n",
    "    \"\"\"\n",
    "    Create individual mental illness predictions for each sample from a specific model.\n",
    "    \"\"\"\n",
    "    # Filter results for this model\n",
    "    model_results = df_results[df_results['model_name'] == model_name].copy()\n",
    "    \n",
    "    # Create mental illness set\n",
    "    model_name_clean = model_name.replace(' ', '_')\n",
    "    illness_set_uri = EX[f\"DepressionSet_{model_name_clean}_{run_id}\"]\n",
    "    graph.add((illness_set_uri, RDF.type, MIAO.MentalIllnessesSet))\n",
    "    graph.add((illness_set_uri, RDFS.label, \n",
    "               Literal(f\"Depression severity detection results from {model_name}\", lang=\"en\")))\n",
    "    graph.add((illness_set_uri, PROV.wasGeneratedBy, run_uri))\n",
    "    graph.add((run_uri, PROV.generated, illness_set_uri))\n",
    "    \n",
    "    # Define Depression subclass (only once)\n",
    "    depression_class = EX.Depression\n",
    "    if (depression_class, RDFS.subClassOf, MIAO.MentalIllness) not in graph:\n",
    "        graph.add((depression_class, RDFS.subClassOf, MIAO.MentalIllness))\n",
    "        graph.add((depression_class, RDFS.label, Literal(\"Depression\", lang=\"en\")))\n",
    "    \n",
    "    # Create individual predictions\n",
    "    for idx, row in model_results.iterrows():\n",
    "        sample_id = row['sample_id']\n",
    "        predicted_label = row['predicted_label']\n",
    "        confidence = row['confidence']\n",
    "        true_label = row['true_label']\n",
    "        \n",
    "        # Create illness instance\n",
    "        illness_uri = EX[f\"Depression_{model_name_clean}_{run_id}_sample_{sample_id}\"]\n",
    "        graph.add((illness_uri, RDF.type, depression_class))\n",
    "        graph.add((illness_uri, MIAO.belongsToMentalIllnessesSet, illness_set_uri))\n",
    "        graph.add((illness_set_uri, MIAO.hasMentalIllness, illness_uri))\n",
    "        \n",
    "        # Add category reference\n",
    "        category_uri = category_map[predicted_label]\n",
    "        graph.add((illness_uri, MIAO.referredToMentalIllnessCategory, category_uri))\n",
    "        \n",
    "        # Add severity level\n",
    "        severity_name = severity_map[predicted_label]\n",
    "        graph.add((illness_uri, MIAO.hasMentalIllnessLevel, Literal(severity_name)))\n",
    "        \n",
    "        # Add confidence\n",
    "        graph.add((illness_uri, MIAO.hasMentalIllnessDetectionConfidence, \n",
    "                   Literal(float(confidence), datatype=XSD.decimal)))\n",
    "        \n",
    "        # Add sample reference\n",
    "        sample_ref = f\"depression_text_sample_{sample_id}\"\n",
    "        graph.add((illness_uri, MIAO.refersToSample, Literal(sample_ref, datatype=XSD.string)))\n",
    "        \n",
    "        # Add label\n",
    "        is_correct = predicted_label == true_label\n",
    "        status = \"correct\" if is_correct else \"incorrect\"\n",
    "        graph.add((illness_uri, RDFS.label, \n",
    "                   Literal(f\"{severity_name} depression prediction ({status}) by {model_name} for sample {sample_id}\", lang=\"en\")))\n",
    "        \n",
    "        # Add description with text snippet\n",
    "        if sample_id < len(df_dataset):\n",
    "            text_snippet = df_dataset.iloc[sample_id]['text'][:100]\n",
    "            true_severity = severity_map[true_label]\n",
    "            graph.add((illness_uri, DCTERMS.description, \n",
    "                       Literal(f\"Predicted: {severity_name}, True: {true_severity}. Text: '{text_snippet}...'\", lang=\"en\")))\n",
    "    \n",
    "    return illness_set_uri\n",
    "\n",
    "# Create predictions for all models\n",
    "illness_sets = {}\n",
    "\n",
    "for model_name in model_metrics.keys():\n",
    "    illness_set_uri = create_predictions(\n",
    "        g, df_results, df_dataset, model_name, \n",
    "        runs[model_name], run_ids[model_name], category_map\n",
    "    )\n",
    "    illness_sets[model_name] = illness_set_uri\n",
    "    \n",
    "    model_results_count = len(df_results[df_results['model_name'] == model_name])\n",
    "    print(f\"Created {model_results_count} predictions for: {model_name}\")\n",
    "\n",
    "print(f\"\\nTotal prediction sets created: {len(illness_sets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export RDF Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RDF GRAPH STATISTICS\n",
      "==================================================\n",
      "Total triples: 357\n",
      "\n",
      "Triples by type:\n",
      "  Depression: 20\n",
      "  ModelEvaluation: 10\n",
      "  HyperParameter: 8\n",
      "  EvaluationMeasure: 5\n",
      "  MentalIllnessCategory: 4\n",
      "  Software: 2\n",
      "  Implementation: 2\n",
      "  Model: 2\n",
      "  AutomaticMentalIllnessesDetection: 2\n",
      "  Run: 2\n",
      "  MentalIllnessesSet: 2\n",
      "  MentalIllnessesSchema: 1\n",
      "  Dataset: 1\n"
     ]
    }
   ],
   "source": [
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RDF GRAPH STATISTICS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total triples: {len(g)}\")\n",
    "print(f\"\\nTriples by type:\")\n",
    "\n",
    "# Count by type\n",
    "type_counts = {}\n",
    "for s, p, o in g.triples((None, RDF.type, None)):\n",
    "    obj_str = str(o).split('#')[-1].split('/')[-1]\n",
    "    type_counts[obj_str] = type_counts.get(obj_str, 0) + 1\n",
    "\n",
    "for type_name, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {type_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDF graph exported to: depression_severity_experiment.ttl\n",
      "File size: 23471 bytes\n"
     ]
    }
   ],
   "source": [
    "# Export to Turtle format\n",
    "output_file = \"depression_severity_experiment.ttl\"\n",
    "g.serialize(destination=output_file, format=\"turtle\")\n",
    "print(f\"\\nRDF graph exported to: {output_file}\")\n",
    "print(f\"File size: {len(g.serialize(format='turtle'))} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sample SPARQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: Model Performance Comparison\n",
      "======================================================================\n",
      "Model: Depression severity detection run: BERT_Depression_Classifier\n",
      "  Accuracy: 0.900\n",
      "  Macro F1: 0.880\n",
      "\n",
      "Model: Depression severity detection run: Feature_Framework_Classifier\n",
      "  Accuracy: 0.850\n",
      "  Macro F1: 0.830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1: Compare model performance\n",
    "query1 = \"\"\"\n",
    "PREFIX mls: <http://www.w3.org/ns/mls#>\n",
    "PREFIX miao: <https://w3id.org/miao#>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?runLabel ?accuracy ?macroF1\n",
    "WHERE {\n",
    "  ?run a miao:AutomaticMentalIllnessesDetection ;\n",
    "       rdfs:label ?runLabel .\n",
    "  \n",
    "  ?run mls:hasOutput ?accEval , ?f1Eval .\n",
    "  \n",
    "  ?accEval mls:specifiedBy ?accMeasure ;\n",
    "           mls:hasValue ?accuracy .\n",
    "  ?accMeasure rdfs:label \"Accuracy\"@en .\n",
    "  \n",
    "  ?f1Eval mls:specifiedBy ?f1Measure ;\n",
    "          mls:hasValue ?macroF1 .\n",
    "  ?f1Measure rdfs:label \"Macro F1 Score\"@en .\n",
    "}\n",
    "ORDER BY DESC(?accuracy)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Query 1: Model Performance Comparison\")\n",
    "print(\"=\"*70)\n",
    "results = g.query(query1)\n",
    "for row in results:\n",
    "    print(f\"Model: {row.runLabel}\")\n",
    "    print(f\"  Accuracy: {float(row.accuracy):.3f}\")\n",
    "    print(f\"  Macro F1: {float(row.macroF1):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: Prediction Distribution by Severity Level\n",
      "======================================================================\n",
      "Mild: <built-in method count of ResultRow object at 0x7c68705d18f0> predictions\n",
      "Minimal: <built-in method count of ResultRow object at 0x7c68708347c0> predictions\n",
      "Moderate: <built-in method count of ResultRow object at 0x7c68705d18f0> predictions\n",
      "Severe: <built-in method count of ResultRow object at 0x7c68708347c0> predictions\n"
     ]
    }
   ],
   "source": [
    "# Query 2: Distribution of predictions by severity level\n",
    "query2 = \"\"\"\n",
    "PREFIX miao: <https://w3id.org/miao#>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "SELECT ?severityLevel (COUNT(?illness) as ?count)\n",
    "WHERE {\n",
    "  ?illness miao:hasMentalIllnessLevel ?severityLevel .\n",
    "}\n",
    "GROUP BY ?severityLevel\n",
    "ORDER BY ?severityLevel\n",
    "\"\"\"\n",
    "\n",
    "print(\"Query 2: Prediction Distribution by Severity Level\")\n",
    "print(\"=\"*70)\n",
    "results = g.query(query2)\n",
    "for row in results:\n",
    "    print(f\"{row.severityLevel}: {row.count} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 3: High-Confidence Severe Depression Cases\n",
      "======================================================================\n",
      "Sample: depression_text_sample_3\n",
      "  Confidence: 0.910\n",
      "\n",
      "Sample: depression_text_sample_7\n",
      "  Confidence: 0.880\n",
      "\n",
      "Sample: depression_text_sample_3\n",
      "  Confidence: 0.870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 3: High confidence Severe depression predictions\n",
    "query3 = \"\"\"\n",
    "PREFIX miao: <https://w3id.org/miao#>\n",
    "PREFIX ex: <https://w3id.org/miao/depression-experiment#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT ?illness ?confidence ?sample\n",
    "WHERE {\n",
    "  ?illness a ex:Depression ;\n",
    "           miao:referredToMentalIllnessCategory ex:Severe ;\n",
    "           miao:hasMentalIllnessDetectionConfidence ?confidence ;\n",
    "           miao:refersToSample ?sample .\n",
    "  \n",
    "  FILTER(?confidence >= 0.85)\n",
    "}\n",
    "ORDER BY DESC(?confidence)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nQuery 3: High-Confidence Severe Depression Cases\")\n",
    "print(\"=\"*70)\n",
    "results = g.query(query3)\n",
    "for row in results:\n",
    "    print(f\"Sample: {row.sample}\")\n",
    "    print(f\"  Confidence: {float(row.confidence):.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 4: Model Disagreements on Severity Level\n",
      "======================================================================\n",
      "Sample: depression_text_sample_7\n",
      "  Model 1: Severe (confidence: 0.880)\n",
      "  Model 2: Moderate (confidence: 0.750)\n",
      "\n",
      "Sample: depression_text_sample_7\n",
      "  Model 1: Moderate (confidence: 0.750)\n",
      "  Model 2: Severe (confidence: 0.880)\n",
      "\n",
      "Total disagreements: 2\n"
     ]
    }
   ],
   "source": [
    "# Query 4: Compare predictions between models for same samples\n",
    "query4 = \"\"\"\n",
    "PREFIX miao: <https://w3id.org/miao#>\n",
    "PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "\n",
    "SELECT ?sample \n",
    "       ?severity1 ?confidence1 \n",
    "       ?severity2 ?confidence2\n",
    "WHERE {\n",
    "  # First model prediction\n",
    "  ?illness1 miao:hasMentalIllnessLevel ?severity1 ;\n",
    "            miao:hasMentalIllnessDetectionConfidence ?confidence1 ;\n",
    "            miao:refersToSample ?sample ;\n",
    "            miao:belongsToMentalIllnessesSet ?set1 .\n",
    "  \n",
    "  # Second model prediction for same sample\n",
    "  ?illness2 miao:hasMentalIllnessLevel ?severity2 ;\n",
    "            miao:hasMentalIllnessDetectionConfidence ?confidence2 ;\n",
    "            miao:refersToSample ?sample ;\n",
    "            miao:belongsToMentalIllnessesSet ?set2 .\n",
    "  \n",
    "  FILTER(?set1 != ?set2)\n",
    "  FILTER(?illness1 != ?illness2)\n",
    "  FILTER(?severity1 != ?severity2)  # Only show disagreements\n",
    "}\n",
    "ORDER BY ?sample\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nQuery 4: Model Disagreements on Severity Level\")\n",
    "print(\"=\"*70)\n",
    "results = g.query(query4)\n",
    "disagreement_count = 0\n",
    "for row in results:\n",
    "    disagreement_count += 1\n",
    "    print(f\"Sample: {row.sample}\")\n",
    "    print(f\"  Model 1: {row.severity1} (confidence: {float(row.confidence1):.3f})\")\n",
    "    print(f\"  Model 2: {row.severity2} (confidence: {float(row.confidence2):.3f})\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total disagreements: {disagreement_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MIAO MODELING VALIDATION REPORT\n",
      "======================================================================\n",
      "[PASS] Mental Illness Schema: 1/1\n",
      "[PASS] Mental Illness Categories: 4/4\n",
      "[PASS] Dataset: 1/1\n",
      "[PASS] ML Implementations: 2/2\n",
      "[PASS] Trained Models: 2/2\n",
      "[PASS] Detection Runs: 2/2\n",
      "[PASS] Mental Illness Sets: 2/2\n",
      "[PASS] Individual Predictions: 20/20\n",
      "[PASS] Model Evaluations: 10/10\n",
      "\n",
      "======================================================================\n",
      "VALIDATION PASSED: All MIAO components correctly modeled\n",
      "======================================================================\n",
      "\n",
      "Additional Statistics:\n",
      "  Number of models: 2\n",
      "  Number of samples: 10\n",
      "  Total predictions: 20\n",
      "  Average predictions per model: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Generate validation report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MIAO MODELING VALIDATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check required components\n",
    "num_models = len(model_metrics)\n",
    "num_samples = len(df_dataset)\n",
    "num_predictions = len(df_results)\n",
    "\n",
    "checks = [\n",
    "    (\"Mental Illness Schema\", len(list(g.triples((None, RDF.type, MIAO.MentalIllnessesSchema)))), 1),\n",
    "    (\"Mental Illness Categories\", len(list(g.triples((None, RDF.type, MIAO.MentalIllnessCategory)))), 4),\n",
    "    (\"Dataset\", len(list(g.triples((None, RDF.type, MLS.Dataset)))), 1),\n",
    "    (\"ML Implementations\", len(list(g.triples((None, RDF.type, MLS.Implementation)))), num_models),\n",
    "    (\"Trained Models\", len(list(g.triples((None, RDF.type, MLS.Model)))), num_models),\n",
    "    (\"Detection Runs\", len(list(g.triples((None, RDF.type, MIAO.AutomaticMentalIllnessesDetection)))), num_models),\n",
    "    (\"Mental Illness Sets\", len(list(g.triples((None, RDF.type, MIAO.MentalIllnessesSet)))), num_models),\n",
    "    (\"Individual Predictions\", len(list(g.triples((None, MIAO.hasMentalIllnessDetectionConfidence, None)))), num_predictions),\n",
    "    (\"Model Evaluations\", len(list(g.triples((None, RDF.type, MLS.ModelEvaluation)))), num_models * 5),\n",
    "]\n",
    "\n",
    "all_passed = True\n",
    "for component, actual, expected in checks:\n",
    "    status = \"PASS\" if actual >= expected else \"FAIL\"\n",
    "    if status == \"FAIL\":\n",
    "        all_passed = False\n",
    "    print(f\"[{status}] {component}: {actual}/{expected}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if all_passed:\n",
    "    print(\"VALIDATION PASSED: All MIAO components correctly modeled\")\n",
    "else:\n",
    "    print(\"VALIDATION FAILED: Some components are missing or incomplete\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"  Number of models: {num_models}\")\n",
    "print(f\"  Number of samples: {num_samples}\")\n",
    "print(f\"  Total predictions: {num_predictions}\")\n",
    "print(f\"  Average predictions per model: {num_predictions / num_models:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
